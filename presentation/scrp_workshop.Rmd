---
title: "Automated Web Scraping with R"
subtitle: "Resul Umit"
institute: ""
date: "May 2021"
output:
  xaringan::moon_reader:
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      highlightSpans: true
      countIncrementalSlides: false
      ratio: "16:9"
    seal: false
---

class: inverse, center, middle

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE, crayon.enabled = TRUE)
library(crayon)
library(fansi)
library(dplyr)
library(ggplot2)
library(stargazer)
library(countdown)
```

<style type="text/css">

.hljs-github .hljs {
    background: #e5e5e5;
}

.inline-c, remark-inline-code {
   background: #e5e5e5;
   border-radius: 3px;
   padding: 4px;
   font-family: 'Source Code Pro', 'Lucida Console', Monaco, monospace;
}


.yellow-h{
   background: #ffff88;
}


.out-t, remark-inline-code {
   background: #9fff9f;
   border-radius: 3px;
   padding: 4px;
   
}

.pull-left-c {
    float: left;
    width: 58%;
    
}

.pull-right-c {
    float: right;
    width: 38%;
    
}

.medium {
    font-size: 75%
    
}

.small {
    font-size: 50%
    }

.action {
    background-color: #f2eecb;
  
}


</style>


# Automated Web Scraping with R

<br>

### Resul Umit

### May 2021

.footnote[

[Skip intro &mdash; To the contents slide](#contents-slide).
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <a href="mailto:resuluy@uio.no?subject=Twitter workshop">I can teach this workshop at your institution &mdash; Email me</a>.

]

---
## Who am I?

Resul Umit

- post-doctoral researcher in political science at the University of Oslo

- teaching and studying representation, elections, and parliaments
    - [a recent publication](https://doi.org/10.1177%2F1478929920967588):
    Parliamentary communication allowances do not increase electoral turnout or incumbents’ vote share

--

<br>

- teaching workshops, also on

  - [writing reproducible research papers](https://resulumit.com/blog/rmd-workshop/)
  - [version control and collaboration](https://resulumit.com/teaching/git_workshop.html)
  - [working with Twitter data](https://resulumit.com/teaching/twtr_workshop.html)
  - [creating academic websites](https://resulumit.com/teaching/rbd_workshop.html)
    
--

<br>

- more information available at [resulumit.com](https://resulumit.com/)

---
## The Workshop &mdash; Overview

- One day, on how to automate the process of extracting data from websites

  - 200+ slides, 75+ exercises
  - a [demonstration website](https://parliament-luzland.netlify.app/) for practice

--

<br>

- Designed for researchers with basic knowledge of R programming language

  - does not cover programming with R
      - e.g., we will use existing functions and packages   
<br>
  - ability to work with R will be very helpful
      - but not absolutely necessary &mdash; this ability can be developed during and after the workshop as well
        
---
## The Workshop &mdash; Motivation

- Data available on websites provide attractive opportunities for academic research

  - e.g., parliamentary websites were the main source of data for my PhD

--

<br>

- Acquiring such data requires 

  - either a lot of resources, such as time
  - or a set of skills, such as automated web scraping

--

<br>

- Typically, such skills are not part of academic training

  - for my PhD, I hand-visited close to 3000 webpages to collect data manually
      - on members of ten parliaments
      - multiple times, to update the dataset as needed

---
## The Workshop &mdash; Motivation &mdash; Aims

- To provide you with an understanding of what is ethically possible

  - we will cover a large breath of issues, not all of it is for long-term memory
      - hence the slides are designed for self study as well    
<br>
  - awareness of what is ethical and possible, `Google`, and perseverance are all you need

--

<br>

- To start you with acquiring and practicing the skills needed 

  - practice with the demonstration website
     - plenty of data, stable structure, and an ethical playground
  - start working on a real project

---
name: contents-slide

## The Workshop &mdash; Contents

.pull-left[

[Part 1. Getting the Tools Ready](#part1)
   - e.g., installing packages
   
[Part 2. Preliminary Considerations](#part2)
   - e.g., ethics of web scraping


[Part 3. Data Collection](#part3)
   - e.g., acquiring a user's tweets
   
[Part 4. Data Preperation](#part4)
   - e.g., creating a tidy dataset of tweets
   
]

.pull-right[
   

   
]

.footnote[

[To the list of references](#reference-slide).

] 

---
## The Workshop &mdash; Organisation

- ~~Sit in groups of two~~ Breakout in groups of two for exercises

  - participants learn as much from their partner as from instructors
  - switch partners after every other part
  - leave your breakout room manually, when everyone in the group is ready

<br> 

- Type, rather than copy and paste, the code that you will find on these slides

  - typing is a part of the learning process
  - slides are, and will remain, available at [resulumit.com/teaching/scrp_workshop.html](https://resulumit.com/teaching/scrp_workshop.html)

<br> 

- When you have a question

  - ask your partner
  - google together
  - ask me

---
class: action

## The Workshop &mdash; Organisation &mdash; Slides

Slides with this background colour indicate that your action is required, for

- setting the workshop up
    - e.g., installing R 
    
- completing the exercises
    - e.g., downloading tweets
    - there are 75+ exercises
    - these slides have countdown timers
        - as a guide, not to be followed strictly
    
`r countdown(minutes = 3, seconds = 00, top = 0)`

---
## The Workshop &mdash; Organisation &mdash; Slides

- Codes and texts that go in R console or scripts .inline-c[appear as such &mdash; in a different font, on gray background]    

    - long codes and texts will have their own line(s)

```{r, demonstration, eval=FALSE}
# read in  the tweets dataset
df <- read_rds("tweets.rds") %>%
  
# split the variable text, create a new variable called da_tweets   
  unnest_tokens(output = da_tweets, input = text, token = "tweets") %>%
 
# remove rows that match any of the stop words as stored in the stop_words dataset 
  anti_join(stop_words, by = c("da_tweets" = "word")) 
```

---
## The Workshop &mdash; Organisation &mdash; Slides

- Codes and texts that go in R console or scripts .inline-c[appear as such &mdash; in a different font, on gray background]    

    - long codes and texts will have their own line(s)

<br>

- Results that come out as output .out-t[appear as such &mdash; in the same font, on green background]    

    - except very obvious results, such as figures and tables
    
--

<br>

    
- Specific sections are .yellow-h[highlighted yellow as such] for emphasis

    - these could be for anything &mdash; codes and texts in input, results in output, and/or texts on slides
    
--

<br>

- The slides are designed for self-study as much as for the workshop

    - *accessible*, in substance and form, to go through on your own

---
name: part1
class: inverse, center, middle

# Part 1. Getting the Tools Ready

.footnote[

[Back to the contents slide](#contents-slide).

]

---
class: action

## Workshop Slides &mdash; Access on Your Browser

- Having the workshop slides<sup>*</sup> on your own machine might be helpful

  - flexibility to go back and forward on your own
      - especially while in a breakout room
  - ability to scroll across long codes on some slides

<br>

- Access at <https://resulumit.com/teaching/scrp_workshop.html>

  - will remain accessible after the workshop
  - might crash for some Safari users
      - if using a different browser application is not an option, view the [PDF version of the slides](https://github.com/resulumit/twtr_workshop/blob/master/presentation/twtr_workshop.pdf) on GitHub
  
.footnote[

<sup>*</sup> These slides are produced in R, with the `xaringan` package ([Xie, 2020](https://cran.r-project.org/web/packages/xaringan/xaringan.pdf)).

]

---
name: download-zip
class: action

## Course Materials &mdash; Download from the Internet

- Download the materials from <https://github.com/resulumit/twtr_workshop/tree/materials>

  - on the webpage, follow

> `Code -> Download ZIP`

<br>

- Unzip and rename the folder

  - unzip to a location that is not synced
     - e.g., perhaps to *Documents*, but not Dropbox

---
## Course Materials &mdash; Overview

Materials have the following structure

```

twtr_workshop-materials
   |
   |- data
   |  |
   |  |- mps.csv
   |  |- status_ids.rds
   |  |- tweets.rds
   |
   |- analysis
   |  |
   |  |- tweet_based.Rmd
   |  |- tweet_based_answers.Rmd
   |  |- user_based.Rmd
   |  |- user_based_answers.Rmd

```
---
## Course Materials &mdash; Contents

- `data/mps.csv`

  - a dataset on the members of parliament (MPs) in the British House of Commons, at the end of January 2021
  - it includes variables on electoral results as well as Twitter usernames

<br>

- `data/status_ids.rds`

  - a dataset with a single variable: `status_id`
  - lists the status IDs of all tweets posted by the MPs listed in `mps.csv`, during January 2021
  
<br>

- `data/tweets.rds`

  - similar to `data/status_ids`, except that
      - the time period is now limited to 15 to 31 January, reducing the number of observations below 50,000, allowing for all variables to be posted online

---
## Course Materials &mdash; Contents

- `tweet_based.Rmd`

  - an R Markdown file with exercises for [Part 6](#part-6)
  - the solution to these exercises are in `tweet_based_answers.Rmd`

<br>

- `user_based.Rmd`

  - an R Markdown file with exercises for [Part 5](#part-5)
  - the solution to these exercises are in `user_based_answers.Rmd`

---
class: action

## R &mdash; Download from the Internet and Install

- Programming language of this workshop

  - created for data analysis, extending for other purposes
      - e.g., accessing APIs
  - allows for all three steps in one environment
      - collecting, processing, and analysing Twitter data
  - an alternative: [python](https://www.python.org/)

<br>

- Optional, if you have R already installed

  - consider updating your copy, if it is not up to date
     - type the `R.version.string` command in R to check the version of your copy
     - compare with the latest official release at [https://cran.r-project.org/sources.html](https://cran.r-project.org/sources.html)

<br>

- Download R from [https://cloud.r-project.org](https://cloud.r-project.org)

    - choose the version for your operating system

---
class: action

## RStudio &mdash; Download from the Internet and Install

- Optional, but highly recommended

  - facilitates working with Twitter data in R

<br>

- A popular integrated development environment (IDE) for R

  - an alternative: [GNU Emacs](https://www.gnu.org/software/emacs/)

<br>

- Download RStudio from [https://rstudio.com/products/rstudio/download](https://rstudio.com/products/rstudio/download)

  - choose the free version
  - consider updating your copy, if it is not up to date, following from the RStudio menu:

> `Help -> Check for Updates`

---
class: action
name: rstudio-project

## RStudio Project &mdash; Create from within RStudio 

- RStudio allows for dividing your work with R into separate projects

  - each project gets dedicated workspace, history, and source documents
  - [this page](https://support.rstudio.com/hc/en-us/articles/200526207-Using-Projects) has more information on why projects are recommended

<br>

- Create a new RStudio project for the existing<sup>*</sup> workshop directory `...\twtr_workshop-materials` from the RStudio menu:

> `File -> New Project -> Existing Directory -> Browse -> ...\twtr_workshop-materials -> Open`


.footnote[

<sup>*</sup> Recall that we have downloaded this earlier from GitHub. [Back to the relevant slide](#download-zip).

]

---
class: action

## R Packages &mdash; Install from within RStudio<sup>*</sup>

```{r eval=FALSE, tidy=FALSE}
install.packages(c("rtweet", "httpuv", "tidyverse", "tidytext"))
```


.footnote[

<sup>*</sup> You may already have a copy of one or more of these packages. In that case, I recommend updating by re-installing them now.

]

---
class: action

## R Packages &mdash; Install from within RStudio

```{r eval=FALSE, tidy=FALSE}
install.packages(c("rtweet", "httpuv", "tidyverse", "tidytext"))
```

- `rtweet` ([Kearney, 2020](https://cran.r-project.org/web/packages/rtweet/index.html)), for collecting tweets

    - alternatives: used to be `twitteR` ([Gentry, 2015](https://cran.r-project.org/web/packages/twitteR/index.html)), but not anymore; [running Python code in R](https://blog.twitter.com/developer/en_us/topics/tips/2020/running-the-python-package-for-search-tweets-in-r.html)

--
<br>

- `httpuv` ([Cheng & Chang, 2020](https://cran.r-project.org/web/packages/httpuv/index.html)), for API authorization

    - alternative: using your own access tokens
        - necessitates making an application through a developer
        - has advantages that we will discuss later on

---
class: action

## R Packages &mdash; Install from within RStudio

```{r eval=FALSE, tidy=FALSE}
install.packages(c("rtweet", "httpuv", "tidyverse", "tidytext"))
```

- `tidyverse` ([Wickham & RStudio 2019](https://cran.r-project.org/web/packages/tidyverse/index.html)), for various tasks

    - including data manipulation, visualisation
    - alternative: e.g., `base` R

--
<br>

- `tidytext` ([Robinson & Silge, 2021](https://cran.r-project.org/web/packages/tidytext/index.html)), for working with text as data

    - alternative: e.g., `quanteda` ([Benoit et al., 2020](https://cran.r-project.org/web/packages/quanteda/index.html))

---
## Twitter &mdash; Authorisation

Authorization to use Twitter APIs requires at least three steps<sup>*</sup>

1) open a user account on Twitter
  - a personal or an institutional (perhaps, for a research project) one
  - done once, takes minutes
      
2) with that user account, apply for a developer account
  - so that you are recognised as a developer, have access to the [developer portal](https://developer.twitter.com/en/portal)
  - done once per account, .yellow-h[takes days to get approved manually]
      
3) with that developer account, register a Twitter app
  - so that you have the keys and tokens for authorisation
  - repeated for every project, takes minutes
      
      
.footnote[

<sup>*</sup> There may be additional steps, such as registering for the [Academic Research product track](https://developer.twitter.com/en/portal/petition/academic/is-it-right-for-you).

]

---
## Twitter &mdash; Authorisation &mdash; Notes

- It is possible to interact with Twitter APIs without steps 2 and 3

  - `rtweet` has a its own Twitter app &mdash; `rstats2twitter` &mdash; that anyone can use
      - anyone with a Twitter account, who authorises `rstats2twitter` via a pop-up browser

--

<br>

- I recommend
   
   - following only the step 1 (open an account) now, which
       - you might already have done
       - is otherwise automatic
       - allows us to use `rstats2twitter` and follow the workshop   
<br>
   - leaving the remaining steps until [Part 8](#part-8)
       - to allow you to think and write your applications carefully
       - to get my feedback if you prefer to do so
       
---
class: action

## Twitter &mdash; Open an Account

Sign up for Twitter at [https://twitter.com/](twitter.com)

- a pre-condition for interacting with Twitter APIs

    - e.g., you must be authorized 
       - even to use `rtweet`'s app &mdash; `rstats2twitter`
    
<br>
- helpful for getting to know what you study

    - e.g., the written and unwritten rules that mediate the behaviour on Twitter
        - as discussed [in Part 1](#potential-biases)

<br>
- with a strategic username

  - usernames are changeable, but nevertheless public
      - either choose an anonymous username (e.g., `asdf029348`)
      - or choose one carefully &mdash; they become a part of users' online presence

---
## Other Resources<sup>*</sup>

- R for Data Science ([Wickham & Grolemund, 2019](https://r4ds.had.co.nz/))
  
   - open access at [https://r4ds.had.co.nz](https://r4ds.had.co.nz/)
   
--

<br>

- Text Mining with R: A Tidy Approach ([Silge & Robinson, 2017](https://www.tidytextmining.com/))
 
   - open access at [tidytextmining.com](https://www.tidytextmining.com/)
   - comes with [a course website](https://juliasilge.shinyapps.io/learntidytext/) where you can practice
   
--

<br>

- A Tutorial for Using Twitter Data in the Social Sciences: Data Collection, Preparation, and Analysis ([Jürgens & Jungherr, 2016](http://dx.doi.org/10.2139/ssrn.2710146))

   - open access at [http://dx.doi.org/10.2139/ssrn.2710146](http://dx.doi.org/10.2139/ssrn.2710146)
   
   
.footnote[

<sup>*</sup> I recommend these to be consulted not during but after the workshop.

]

---
name: part2
class: inverse, center, middle

# Part 2. Preliminary Considerations

.footnote[

[Back to the contents slide](#contents-slide).

]

---
## Considerations &mdash; Law

- Web scraping might be illegal
   - 




---
## Considerations &mdash; Research Questions & Hypotheses

- Ideally, we have one or more research questions, hypotheses

  - developed prior to data collection, analysis
     - based on, e.g., theory, claims, observations   
<br>
  - perhaps, even pre-registered
     - e.g., at [OSF Registries](https://osf.io/registries)

--

<br>

- Not all questions can be answered with Twitter data

  - see relevant literature for what works, what does not
     - e.g., for political science, the review by [Jungherr (2016)](https://doi.org/10.1080/19331681.2015.1132401)
        - for public health, the review by [Sinnenberg et al. (2017)](https://doi.org/10.2105/AJPH.2016.303512)

---
## Considerations &mdash; Potential Biases

There are at least two potential sources of bias in Twitter data

- sampling

  - Twitter users are not representative of the people out there
     - see, for example, [Mellon and Prosser (2017)](https://doi.org/10.1177%2F2053168017720008)    
<br>
  - Tweeting behaviour has a strategic component 
     - see, for example, [Umit (2017)](https://doi.org/10.1080/13572334.2017.1283166)


--
name: potential-biases

- mediation

  - the behaviour on Twitter is mediated through written and unwritten rules
     - e.g., there is a button to like, but no dislike
         - might systematically bias the replies towards negative   
<br>
     - e.g., the common use of the like function as a bookmark
         - what would a study of Twitter likes be measuring?

---
## Considerations &mdash; Constraints over Data Access

- Twitter has restrictions on data access

  - how much data is available to download
  - how quickly, how frequently, how far dating back *etc*.
     
--
<br>

- These restrictions vary across API types

  - e.g., [Standard v1.1](https://developer.twitter.com/en/docs/twitter-api/v1) is the most restrictive APIs
      - other first generation APIs are the [Premium v1.1](https://developer.twitter.com/en/products/twitter-api/premium-apis) and [Enterprise: Gnip 2.0](https://developer.twitter.com/en/products/twitter-api/enterprise) APIs &mdash; both with paid subscriptions
      - there are also the second generation APIs, including the newly announced [Academic Research product track](https://blog.twitter.com/developer/en_us/topics/tips/2021/enabling-the-future-of-academic-research-with-the-twitter-api.html)

--
<br>

- These restrictions also vary within APIs types, across different operations

  - e.g., collecting tweets in real time *vs*. collecting historical tweets
     - but also, collecting historical tweets from a specific user *vs*. tweets from any user

---
## Considerations &mdash; Constraints over Data Redistribution

- Twitter restricts content redistribution

   - e.g., only the tweet and/or user IDs can be made publicly available in datasets over 50,000 observations
      - e.g., not the tweets themselves
      - and no more than 1.5M IDs
         - with some exceptions for academic research   
<br>
   - see [Twitter Developer terms](https://developer.twitter.com/en/developer-terms/agreement-and-policy) for further details   

--
<br>

- Reproducibility of research based on Twitter data is limited in practice

  - i.e., reproducibility after publication, by others
  - technically, they can retrieve the same tweets with IDs
      - demanding for reproducers
      - may even be impossible
          - e.g., some tweets, or whole accounts, might be deleted before replication attempts

---
## Considerations &mdash; Changes in the Twitter APIs

- Twitter is currently switching to a new generation of APIs

  - replacing APIs v1 with v2
      - each with various types of APIs   
<br>
  - the switch is not complete, outcome is not clear
     - see the [early access](https://developer.twitter.com/en/docs/twitter-api/early-access) options
     
--
<br>

- Twitter might change the rules of the APIs game at any anytime, again

  - making the existing restrictions more or less strict
     - e.g., while you are in the middle of data collection     
<br>
  - breaking your plans, code

  
---
## Considerations &mdash; Changes in the Twitter APIs &mdash; Notes

- Existing codes to collect tweets may or may not be affected, depending on

   - how the APIs v2 will look in the end
       - it is still a work in progress   
<br>
   - how the `rtweet` package<sup>*</sup> will adopt
       - it is currently going through a major revision


.footnote[

<sup>*</sup> This is the R package that we will use to collect tweets. More details are in [Part 2](#part2). 

]   

---
## Considerations &mdash; Changes in the Twitter APIs &mdash; Notes

- Existing codes to collect tweets may or may not be affected, depending on

   - how the APIs V2 will look in the end
       - it is still a work in progress   
<br>
   - whether and how the `rtweet` package will adopt
       - it is currently going through a major revision

<br>

- Not all changes are bad

   - among others, APIs v2 introduces the [Academic Research product track](https://blog.twitter.com/developer/en_us/topics/tips/2021/enabling-the-future-of-academic-research-with-the-twitter-api.html)    
<br>
       - 'to serve the unique needs and challenges of academic researchers'
           - ranging from master's students to professors    
<br>
       - access to all public tweets
          - by up to 1.5M a month at a time
       
---
## Considerations &mdash; Law and Ethics

- It is often impossible to get users' consent

  - i.e., for collecting and analysing their data on Twitter    
<br>
  - Twitter itself has no problem with it, but others might disagree
      - e.g., your law makers, (funding and/or research) institution, subjects, conscience
    
--

<br>

- Check the rules that apply to your case
   
  - rules and regulations in your country, at your institution
  
--

<br>

- Reflect on whether using Twitter data for research is ethical

  - even where it is legal and allowed, it may not be moral

---
## Considerations &mdash; Data Storage

Twitter data frequently requires 

- large amounts of digital storage space

  - Twitter data is typically big data
      - many tweets, up to 90 variables   
<br>
  - e.g., a dataset of 1M tweets requires about 300MB 
      - when stored in R data formats

--

- private, safe storage spaces

  - due to [Twitter Developer terms](https://developer.twitter.com/en/developer-terms/agreement-and-policy)
  - but also local rules, institutional requirements

---
## Considerations &mdash; Language and Context

- Some tools of text analysis are developed for a specific language and/or context

   - e.g., dictionaries for sentiment analysis
       - might be in English, for political texts, only
       
   - these may not be useful, valid for different languages, and/or contexts
   
--

<br>

- Some tools of text analysis are developed for general use

   - e.g., a dictionary for sentiments in everyday language
   
   - these may not be useful, valid for a specific context
      - e.g., political texts

---
name: part3
class: inverse, center, middle

# Part 3. Data Collection

.footnote[

[Back to the contents slide](#contents-slide).

]

---
name: part4
class: inverse, center, middle

# Part 4. Data Preperation

.footnote[

[Back to the contents slide](#contents-slide).

]

---
## Data Preperation &mdash; Overview

- The `rtweet` package does a very good job with data preperation to start with

   - returns data frames, with mostly tidy data
   - although Twitter APIs return nested lists
   - some variables are still lists
      - e.g., `hastags`

--

<br>

- Further data preparation depends on your research project

   - most importantly, on whether you will work with texts or not
   - we will cover some common preparation steps

---
## Data Preperation &mdash; Overview &mdash; Strings

- Most researchers would be interested in textual Twitter data

   - tweets as a whole, but also specifically hashtags *etc*.

--

<br>

- There are many components of tweets as texts

   - e.g., mentions, hashtags, emojis, links *etc*.
   - but also punctuation, white spaces, upper case letters *etc*.
   - some of these may need to be taken out before analysis

--

<br>

- I use the `stringr` package ([Wickham, 2019](https://cran.r-project.org/web/packages/stringr/index.html)) for string operations

   - part of the `tidyverse` family
   - you might have another favourite already
       - no need to change as long as it does the job
       
---
## Data Preperation &mdash; Overview &mdash; Numbers

- There is more to Twitter data than just tweets
      
  - e.g., the number of followers, likes *etc*.
     - see Silva and Proksch ([2020](https://doi.org/10.1017/S0003055420000817)) for a great example

--
<br>

- I use the `dplyr` package ([Wickham et al, 2020](https://cran.r-project.org/web/packages/dplyr/index.html) for most data operations

   - part of the `tidyverse` family
   - you might have another favourite already
       - no need to change as long as it does the job

---
## Data Preperation &mdash; Remove Mentions

```{r, eval=FALSE}
tweet <- "These from @handle1 are #socool. 👏 A #mustsee, @handle2! 
          👉 t.co/aq7MJJ1
          👉 https://t.co/aq7MJJ2"
````

```{r, eval=FALSE}
str_remove_all(string = tweet, pattern = "[@][\\w_-]+")
```


.out-t[

[1] "This from  are #socool. 👏 A #mustsee, ! 
          👉 t.co/aq7MJJ1
          👉 https://t.co/aq7MJJ2"

]


---
## Data Preperation &mdash; Remove Hashtags


```{r, eval=FALSE}
tweet <- "These from @handle1 are #socool. 👏 A #mustsee, @handle2! 
          👉 t.co/aq7MJJ1
          👉 https://t.co/aq7MJJ2"
````

```{r, eval=FALSE}
str_remove_all(string = tweet, pattern = "[`#`][\\w_-]+")
```


.out-t[

[1] "These from @handle1 are . 👏 A , @handle2! 
          👉 t.co/aq7MJJ1
          👉 https://t.co/aq7MJJ2"

]

---
## Data Preperation &mdash; Remove Links

```{r, eval=FALSE}
tweet <- "These from @handle1 are #socool. 👏 A #mustsee, @handle2! 
          👉 t.co/aq7MJJ1
          👉 https://t.co/aq7MJJ2"

````

```{r, eval=FALSE}
str_remove_all(string = tweet, pattern = "http\\S+\\s*")
```


.out-t[

[1] "These from @handle1 are. 👏 A, @handle2! 
          👉 t.co/aq7MJJ1"

]

<br>

- Notice that

  - links come in various formats
  - you may need multiple or complicated regular expression patterns

---
## Data Preperation &mdash; Remove Links &mdash; Alternative

Use the `urls_t.co` variable to remove all links

- if there are more than one link in a tweet, they are stored as a list in this variable


```{r, eval=FALSE}
# start with your existing dataset of tweets
df_tweets <- df_tweets %>%
  
# limit the operation to within individual tweets  
  group_by(status_id) %>%
  
# create a new variable of tweets without links  
  mutate(tidy_text = 
           
# by removing them from the existing variable `text`           
           str_remove_all(string = text, 
 
# that matches the `urls_t.co` variable, after being collapsed into a string      
           pattern = str_c(unlist(urls_t.co), collapse = "|")))
```

--
class: action

`r countdown(minutes = 8, seconds = 00, top = 0)`

---
## Data Preperation &mdash; Remove Emojis

```{r, eval=FALSE}
tweet <- "These from @handle1 are #socool. 👏 A #mustsee, @handle2! 
          👉 t.co/aq7MJJ1
          👉 https://t.co/aq7MJJ2"
````

```{r, eval=FALSE}
iconv(x = tweet, from = "latin1", to = "ASCII", sub = "")
```


.out-t[

[1] "These from @handle1 are #socool. A #mustsee, @handle2! 
          t.co/aq7MJJ1
          https://t.co/aq7MJJ2"

]


---
## Data Preperation &mdash; Exercises &mdash; Notes


- The exercises in this part are best followed by

  - using `tweets.rds` or similar dataset
  - saving a new variable at every step of preparation
  - observing the newly created variables 
      - to confirm whether the code works as intended

<br>

- The `mutate` function, from the `dplyr` package, can be helpful, as follows

   - recall that `text` is the variable for tweets

```{r, eval=FALSE}

tweets <- read_rds("data/tweets.rds")

clean_tweets <- tweets %>%
  `mutate(no_mentions` = str_remove_all(string = `text`, pattern = "[@][\\w_-]+"))

```

---
class: action

## Exercises


41) Remove mentions

- hint: the pattern is `"[@][\\w_-]+"`

42) Remove hastags
- hint: the pattern is `"[#][\\w_-]+"`

43) Remove links
- by using the links from the `urls_t.co` variable

44) Remove emojis
- pull the help file for the `iconv` function first


`r countdown(minutes = 10, seconds = 00, top = 0)`


---
## Data Preperation &mdash; Remove Punctuations

```{r, eval=FALSE}
tweet <- "These from @handle1 are #socool. 👏 A #mustsee, @handle2! 
          👉 t.co/aq7MJJ1
          👉 https://t.co/aq7MJJ2"
````

```{r, eval=FALSE}
str_remove_all(string = tweet, pattern = "[[:punct:]]")
```


.out-t[

[1] "This from are socool 👏 A mustsee handle2
          👉 tcoaq7MJJ1
          👉 httpst.coaq7MJJ2"

]

Notice that
   - this removed all punctuation, including those in mentions, hashtags, and links
   - if tweets are typed with no spaces after punctuation, this might lead to merged pieces of text
      - alternatively, try the `str_replace` function to replace punctuation with space
      
---
## Data Preperation &mdash; Remove Punctuations &mdash; Alternative

```{r, eval=FALSE}
tweet <- "This is a sentence.There is no space before this sentence."
````

```{r, eval=FALSE}
str_`remove`_all(string = tweet, pattern = "[[:punct:]]")
```

.out-t[

[1] "This is a sentenceThere is no space before this sentence"

]

--

<br>


```{r, eval=FALSE}
str_`replace`_all(string = tweet, pattern = "[[:punct:]]", replacement = " ")
```

.out-t[

[1] "This is a sentence There is no space before this sentence "

]

---
## Data Preperation &mdash; Remove Punctuations &mdash; Alternative


```{r, eval=FALSE}
tweet <- "This is a sentence.There is no space before this sentence."
````

```{r, eval=FALSE}
str_replace_all(string = tweet, pattern = "[[:punct:]]", replacement = " ")
```

.out-t[

[1] "This is a sentence There is no space before this sentence "

]

---
## Data Preperation &mdash; Remove Repeated Whitespace


```{r, eval=FALSE}
tweet <- "There are too many spaces after this sentence.   This is a new sentence."
````

```{r, eval=FALSE}
str_squish(string = tweet)
```

.out-t[

[1] "There are too many spaces after this sentence. This is a new sentence."

]

Note that
- white spaces can be introduced not only by users on Twitter, but also by us, while cleaning the data
   - e.g., removing and/or replacing operations above
   - hence, this function might be useful after other operations

---
## Data Preperation &mdash; Change Case

```{r, eval=FALSE}
tweet <- "lower case. Sentence case. Title Case. UPPER CASE."
````

```{r, eval=FALSE}
str_to_lower(string = tweet)
```

.out-t[

[1] "lower case. sentence case. title case. upper case."

]

Note that
   - there are other functions in this family, including
      - `str_to_sentence`, `str_to_title`, `str_to_upper`


---
class: action

## Exercises

45) Remove punctuations
- by using the `str_replace_all` function
- hint: the pattern is `[[:punct:]]`

<br>

46) Remove whitespace
- hint: the function is called `str_squish`

<br>

47) Change case to lower case
- hint: the function is called `str_to_lower`


`r countdown(minutes = 10, seconds = 00, top = 0)`

---
## Data Preperation &mdash; Change Unit of Observation

Research designs might require changing the unit of observation
   - aggregation
      - e.g., at the level of users, locations, hashtags etc.
      - summarise with `dplyr`
   

  - dis-aggregation
     - e.g., to the level of words
     - tokenise with `tidytext`
     
---
## Data Preperation &mdash; Change Unit of Observation &mdash; Aggregation

Aggregate at the level of users

- the number of tweets per user

```{r, eval=FALSE}
# load the tweets dataset
df <- read_rds("tweets.rds") %>%
  
# group by users for aggregation  
  group_by(user_id) %>%
  
# create summary statistics for variables of interest
  summarise(sum_tweets = n())
```

---
## Data Preperation &mdash; Change Unit of Observation &mdash; Aggregation

What is aggregated at which level depends on your research design, such as
   - aggregate the tweets into a single text
   - at the level of users by source


```{r, eval=FALSE}
# load the tweets dataset
df <- read_rds("tweets.rds") %>%
  
# group by users for aggregation  
  group_by(user_id, `source`) %>%
  
# create summary statistics for variables of interest
  summarise(`merged_tweets = paste0(text, collapse = ". ")`) 
```

---
## Data Preperation &mdash; Change Unit of Observation &mdash; Dis-aggregation

Disaggregate the tweets, by splitting them into smaller units
  - also called .yellow-h[tokenisation]

Note that
- by default `sep = "[^[:alnum:].]+"`, which works well with separating tweets into words
   - change this argument with a regular expression of your choice
- this creates a tidy dataset, where each observation is a word
   - all other tweet-level variables are repeated for each observation

```{r, eval=FALSE}
# load the tweets dataset
df <- read_rds("tweets.rds") %>%
  
# split the variable `text`  
    separate_rows(text)
```

---
## Data Preperation &mdash; Change Unit of Observation &mdash; Dis-aggregation

The `tidytext` has a function that works better with tokenising tweets
- with `token = "tweets"`, it dis-aggregates text into words
   - except that it respects usernames, hashtags, and URLS 

```{r, eval=FALSE}
# load the tweets dataset
df <- read_rds("tweets.rds") %>%
  
# split the variable `text`, create a new variable called `da_tweets`    
    unnest_tokens(output = da_tweets, input = text, token = "tweets")
```

---
## Data Preperation &mdash; Change Unit of Observation &mdash; Dis-aggregation

Tokenise variables to levels other than words
- e.g., characters, words (the default),  sentences, lines

```{r, eval=FALSE}
# load the tweets dataset
df <- read_rds("tweets.rds") %>%
  
# split the variable `text` into sentences, create a new variable called `da_tweets`    
    unnest_tokens(output = da_tweets, input = text, `token = "sentences"`)
```

---
## Data Preperation &mdash; Change Unit of Observation &mdash; Dis-aggregation

Tokenise variables other than tweets
- recall that `rtweet` stores multiple hastags, mentions *etc*. as lists


```{r, eval=FALSE}
# load the tweets dataset
df <- read_rds("tweets.rds") %>%
  
# unlist the lists of hashtags to create strings  
  group_by(status_id) %>%
  mutate(tidy_hashtags = str_c(unlist(hashtags), collapse = " ")) %>%
  
# split the string, create a new variable called `da_tweets`    
  unnest_tokens(output = da_hashtags, input = tidy_hashtags, token = "words")
```

---
## Data Preperation &mdash; Remove Stop Words

Remove the common, uninformative words
- e.g., the, a, i

Note that 
- this operation requires a tokenised-to-word variable
- stop words for English are stored in the `stop_words` dataset in the `tidytext` variable
- list of words for other languages are available elsewhere, including 
   - the `stopwordslangs` function from the `rtweet` package
   - the `stopwords` function from the `tm` package
      - e.g., use `tm::stopwords("german")` for German

```{r, eval=FALSE}
# load the tweets dataset
df <- read_rds("tweets.rds") %>%
  
# split the variable `text`, create a new variable called `da_tweets`    
  unnest_tokens(output = da_tweets, input = text, token = "tweets") %>%
 
# remove rows that match any of the stop words as stored in the stop_words dataset 
  anti_join(stop_words, by = c("da_tweets" = "word"))
```

---
class: action

## Exercises

48) Aggregate `text` to a higher level
- e.g., if you are not using `tweets.rds`, to MP level
   - if not, perhaps to `source` level

<br>

49) Dis-aggregate `text` to a lower level
- e.g., to words

<br>

50) Dis-aggregate `hashtags`
- i.e., make sure each row has at most one hashtag

<br>

51) Remove stop words


`r countdown(minutes = 10, seconds = 00, top = 0)`

---
name: reference-slide
class: inverse, center, middle

# References

.footnote[

[Back to the contents slide](#contents-slide).

]

---
## References

Benoit, K., Watanabe, K., Wang, H., Nulty, P., Obeng, A., Müller, S., Matsuo, A., Lua, J. W., Kuha, J., & Lowe, W. (2020). [quanteda: Quantitative Analysis of Textual Data](https://cran.r-project.org/web/packages/quanteda/index.html). R package, version 2.1.2.

Robinson, D., & Silge, J. (2021). [tidytext: Text mining using 'dplyr', 'ggplot2', and other tidy tools](https://cran.r-project.org/web/packages/tidytext/index.html). R package, version 0.3.0.

Silge, J., & Robinson, D. (2017). [Text mining with R: A tidy approach](https://www.tidytextmining.com). O'Reilly. Open access at

Wickham, H. (2019). [stringr: Simple, Consistent Wrappers for Common String Operations](https://cran.r-project.org/web/packages/stringr/index.html). R package, version 1.4.0.

Wickham, H., Chang, W., Henry, L., Pedersen, T. L., Takahashi, K., Wilke, C., Woo, K., Yutani, H. and Dunnington, D. (2020). [dplyr: A grammar of data manipulation](https://cran.r-project.org/web/packages/dplyr/index.html). R package, version 0.8.5.

Wickham, H. and Grolemund, G. (2019). [R for data science](https://r4ds.had.co.nz). O'Reilly. Open access at [https://r4ds.had.co.nz](https://r4ds.had.co.nz/).

Wickham, H., RStudio (2019). [https://cran.r-project.org/web/packages/tidyverse/index.html](tidyverse: Easily Install and Load the 'Tidyverse'). R package, version 3.3.3.

Xie, Y. (2020). [xaringan: Presentation Ninja](https://cran.r-project.org/web/packages/xaringan/index.html). R package, version 0.19.

---
class: middle, center

## The workshop ends here.
## Congradulations for making it this far, and
## thank you for joining me!

.footnote[

[Back to the contents slide](#contents-slide).

]
